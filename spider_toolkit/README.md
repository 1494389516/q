# ğŸ•·ï¸ çˆ¬è™«ç™¾å®ç®± (Spider Toolkit)

ä¸€ä¸ªåŠŸèƒ½å¼ºå¤§ã€æ˜“äºä½¿ç”¨çš„Pythonç½‘ç»œçˆ¬è™«å·¥å…·åŒ…ï¼Œä¸“ä¸ºé«˜æ•ˆæ•°æ®é‡‡é›†è€Œè®¾è®¡ã€‚

## âœ¨ æ ¸å¿ƒç‰¹æ€§

### ğŸš€ å¼‚æ­¥é«˜æ€§èƒ½
- åŸºäºaiohttpçš„å¼‚æ­¥ç½‘ç»œè¯·æ±‚
- æ™ºèƒ½é‡è¯•æœºåˆ¶å’Œè¯·æ±‚èŠ‚æµ
- æ”¯æŒé«˜å¹¶å‘æ‰¹é‡å¤„ç†
- å†…ç½®è¿æ¥æ± ç®¡ç†

### ğŸ›¡ï¸ åçˆ¬è™«å¯¹ç­–
- æ™ºèƒ½ä»£ç†è½®æ¢ç®¡ç†
- éšæœºè¯·æ±‚å»¶è¿Ÿ
- éªŒè¯ç è‡ªåŠ¨è¯†åˆ«
- è‡ªå®šä¹‰User-Agent

### ğŸ“Š å¤šå¹³å°æ”¯æŒ
- **å°çº¢ä¹¦**: è¯„è®ºã€è§†é¢‘ã€å›¾ç‰‡é‡‡é›†
- **Bç«™**: è¯„è®ºã€è§†é¢‘ä¿¡æ¯è·å–
- **æŠ–éŸ³**: è¯„è®ºæ•°æ®æå–
- **é€šç”¨ç½‘ç«™**: æ™ºèƒ½å†…å®¹è¯†åˆ«

### ï¿½  æ•°æ®ç®¡ç†
- SQLiteæ•°æ®åº“å­˜å‚¨
- JSON/CSVå¤šæ ¼å¼å¯¼å‡º
- é‡å¤æ•°æ®æ£€æµ‹
- å¢é‡æ›´æ–°æ”¯æŒ

## ğŸ“¦ å®‰è£…

```bash
# å…‹éš†é¡¹ç›®
git clone <repository-url>
cd spider_toolkit

# å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### åŸºç¡€ä½¿ç”¨

```python
import asyncio
from spider_toolkit import SpiderToolkit

async def main():
    toolkit = SpiderToolkit()
    
    # çˆ¬å–è¯„è®º
    comments = await toolkit.crawl_comments("https://www.bilibili.com/video/BV1xx411c7mu")
    print(f"è·å–åˆ° {len(comments)} æ¡è¯„è®º")
    
    # ä¸‹è½½è§†é¢‘
    videos = await toolkit.crawl_videos("https://example.com/video-page")
    
    # é‡‡é›†å›¾ç‰‡
    images = await toolkit.crawl_images("https://example.com/gallery")

asyncio.run(main())
```

### å‘½ä»¤è¡Œä½¿ç”¨

```bash
# çˆ¬å–Bç«™è§†é¢‘è¯„è®º
python -m spider_toolkit.cli comments https://www.bilibili.com/video/BV1xx411c7mu -o comments.json

# æ‰¹é‡ä¸‹è½½å›¾ç‰‡
python -m spider_toolkit.cli images https://example.com/gallery --download-dir ./images

# æ‰¹é‡å¤„ç†
python -m spider_toolkit.cli batch urls.txt --type comments --concurrent 5
```

## ğŸ“– è¯¦ç»†åŠŸèƒ½

### 1. è¯„è®ºçˆ¬å–

```python
# Bç«™è¯„è®º
comments = await toolkit.crawl_comments("https://www.bilibili.com/video/BV1xx411c7mu")

# å°çº¢ä¹¦è¯„è®º
comments = await toolkit.crawl_comments("https://www.xiaohongshu.com/explore/xxx")

# è‡ªå®šä¹‰è¯·æ±‚å¤´
headers = {"Cookie": "your_cookie_here"}
comments = await toolkit.crawl_comments(url, headers=headers)
```

### 2. è§†é¢‘ä¸‹è½½

```python
videos = await toolkit.crawl_videos(
    url="https://example.com/video-page",
    download_dir="./downloads/videos"
)

for video in videos:
    print(f"å·²ä¸‹è½½: {video['filename']} ({video['size']} bytes)")
```

### 3. å›¾ç‰‡é‡‡é›†

```python
images = await toolkit.crawl_images(
    url="https://example.com/gallery",
    download_dir="./downloads/images"
)
```

### 4. æ‰¹é‡å¤„ç†

```python
urls = [
    "https://www.bilibili.com/video/BV1xx411c7mu",
    "https://www.bilibili.com/video/BV1yy411c7mu"
]

results = await toolkit.batch_crawl(
    urls=urls,
    crawl_type='comments',
    max_concurrent=5
)

print(f"æˆåŠŸ: {results['success_count']}/{results['total_urls']}")
```

## âš™ï¸ é…ç½®ç®¡ç†

### é…ç½®æ–‡ä»¶ (crawler_config.yaml)

```yaml
request:
  timeout: 30
  max_retries: 3
  delay_range: [1, 3]

proxy:
  enabled: true
  proxy_list:
    - "http://proxy1:8080"
    - "http://proxy2:8080"

captcha:
  enabled: true
  service: "2captcha"
  api_key: "your_api_key"

download:
  base_dir: "./downloads"
  max_file_size: 104857600

database:
  enabled: true
  db_path: "crawler_history.db"
```

### ä»£ç ä¸­é…ç½®

```python
from spider_toolkit import config

# è®¾ç½®ä»£ç†
config.set('proxy.enabled', True)
config.set('proxy.proxy_list', ['http://proxy:8080'])

# è®¾ç½®è¯·æ±‚å»¶è¿Ÿ
config.set('request.delay_range', [2, 5])
```

## ğŸ” éªŒè¯ç å¤„ç†

```python
from spider_toolkit import CaptchaSolver

solver = CaptchaSolver()

# è¯†åˆ«å›¾ç‰‡éªŒè¯ç 
result = solver.solve_image_captcha(image_data)

# å¤„ç†æ»‘å—éªŒè¯ç 
distance = solver.solve_slider_captcha(bg_image, slider_image)

# ä½¿ç”¨2captchaæœåŠ¡
result = solver.solve_with_2captcha(
    api_key="your_api_key",
    image_data=image_bytes
)
```

## ğŸ“Š æ•°æ®å­˜å‚¨

```python
from spider_toolkit import history

# æ£€æŸ¥æ˜¯å¦æœ€è¿‘çˆ¬å–è¿‡
if not history.is_recently_crawled(url, 'comments', hours=24):
    comments = await toolkit.crawl_comments(url)

# è·å–ç»Ÿè®¡ä¿¡æ¯
stats = history.get_statistics()
print(f"æ€»è®¡çˆ¬å–: {stats['total_records']} æ¡è®°å½•")

# å¯¼å‡ºæ•°æ®
history.export_data('export.json', crawl_type='comments')
```

## ğŸ› ï¸ é«˜çº§ç”¨æ³•

### è‡ªå®šä¹‰è§£æå™¨

```python
from spider_toolkit.parsers import BaseParser

class CustomParser(BaseParser):
    async def parse(self, url, html_content, crawler, headers, throttler):
        # è‡ªå®šä¹‰è§£æé€»è¾‘
        return parsed_data

toolkit.comment_parser = CustomParser()
```

### ä»£ç†ç®¡ç†

```python
from spider_toolkit import ProxyManager

proxy_manager = ProxyManager([
    'http://proxy1:8080',
    'http://proxy2:8080'
])

proxy = proxy_manager.get_proxy()
proxy_manager.mark_proxy_failed(proxy)
```

## ğŸ“‹ å‘½ä»¤è¡Œå·¥å…·

```bash
# åŸºæœ¬å‘½ä»¤
spider-toolkit comments <URL> [é€‰é¡¹]
spider-toolkit videos <URL> --download-dir ./videos
spider-toolkit images <URL> --download-dir ./images
spider-toolkit batch urls.txt --type comments --concurrent 5

# é…ç½®ç®¡ç†
spider-toolkit config get request.timeout
spider-toolkit config set proxy.enabled true
spider-toolkit config list

# é«˜çº§é€‰é¡¹
spider-toolkit comments <URL> --proxy http://proxy:8080 --delay 2 5 --verbose
```

## ğŸ“ é¡¹ç›®ç»“æ„

```
spider_toolkit/
â”œâ”€â”€ __init__.py          # åŒ…åˆå§‹åŒ–
â”œâ”€â”€ core.py             # æ ¸å¿ƒçˆ¬è™«ç±»
â”œâ”€â”€ network.py          # ç½‘ç»œè¯·æ±‚æ¨¡å—
â”œâ”€â”€ parsers.py          # å†…å®¹è§£æå™¨
â”œâ”€â”€ database.py         # æ•°æ®åº“ç®¡ç†
â”œâ”€â”€ captcha.py          # éªŒè¯ç å¤„ç†
â”œâ”€â”€ config.py           # é…ç½®ç®¡ç†
â”œâ”€â”€ utils.py            # å·¥å…·å‡½æ•°
â”œâ”€â”€ cli.py              # å‘½ä»¤è¡Œæ¥å£
â””â”€â”€ requirements.txt    # ä¾èµ–åˆ—è¡¨
```

## ğŸ“Š æ”¯æŒå¹³å°

| å¹³å° | è¯„è®º | è§†é¢‘ | å›¾ç‰‡ | ç‰¹æ®ŠåŠŸèƒ½ |
|------|------|------|------|----------|
| å“”å“©å“”å“© | âœ… | âœ… | âœ… | å¼¹å¹•ã€ç”¨æˆ·ä¿¡æ¯ |
| å°çº¢ä¹¦ | âœ… | âœ… | âœ… | ç”¨æˆ·èµ„æ–™ã€æ ‡ç­¾ |
| æŠ–éŸ³ | âœ… | âš ï¸ | âœ… | éŸ³ä¹ä¿¡æ¯ |
| é€šç”¨ç½‘ç«™ | âœ… | âœ… | âœ… | è‡ªå®šä¹‰è§£æ |

## âš ï¸ æ³¨æ„äº‹é¡¹

### ä½¿ç”¨å»ºè®®
- éµå®ˆç½‘ç«™robots.txtåè®®
- åˆç†è®¾ç½®è¯·æ±‚é¢‘ç‡
- å°Šé‡ç½‘ç«™ä½¿ç”¨æ¡æ¬¾
- ä¸é‡‡é›†æ•æ„Ÿä¿¡æ¯

### å¸¸è§é—®é¢˜

**Q: å¦‚ä½•å¤„ç†åçˆ¬è™«ï¼Ÿ**
A: å¯ç”¨ä»£ç†è½®æ¢ã€è°ƒæ•´è¯·æ±‚é¢‘ç‡ã€ä½¿ç”¨éªŒè¯ç è¯†åˆ«

**Q: å¦‚ä½•æé«˜æ•ˆç‡ï¼Ÿ**
A: ä½¿ç”¨æ‰¹é‡å¤„ç†ã€å¢åŠ å¹¶å‘æ•°ã€å¯ç”¨æ•°æ®åº“ç¼“å­˜

**Q: é‡åˆ°åŠ¨æ€å†…å®¹æ€ä¹ˆåŠï¼Ÿ**
A: å¯ç»“åˆSeleniumæˆ–Playwrightè¿›è¡Œæµè§ˆå™¨è‡ªåŠ¨åŒ–

## ğŸ§ª æµ‹è¯•

```bash
# è¿è¡Œæµ‹è¯•
python -m pytest spider_toolkit/tests/ -v

# è¿è¡Œç¤ºä¾‹
python example_usage.py
```

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤Issueå’ŒPull Requestæ¥æ”¹è¿›è¿™ä¸ªé¡¹ç›®ï¼

### å¼€å‘è§„èŒƒ
- éµå¾ªPEP 8ä»£ç é£æ ¼
- æ·»åŠ ç±»å‹æ³¨è§£
- ç¼–å†™å•å…ƒæµ‹è¯•
- æ›´æ–°æ–‡æ¡£

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨MITè®¸å¯è¯ï¼Œè¯¦è§LICENSEæ–‡ä»¶ã€‚

## ğŸ™ è‡´è°¢

æ„Ÿè°¢ä»¥ä¸‹å¼€æºé¡¹ç›®ï¼š
- [aiohttp](https://github.com/aio-libs/aiohttp) - å¼‚æ­¥HTTPå®¢æˆ·ç«¯
- [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/) - HTMLè§£æ
- [Tesseract](https://github.com/tesseract-ocr/tesseract) - OCRå¼•æ“

---

â­ **å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹ä½ æœ‰å¸®åŠ©ï¼Œè¯·ç»™ä¸ªStaræ”¯æŒä¸€ä¸‹ï¼**

**å…è´£å£°æ˜**: æœ¬å·¥å…·ä»…ä¾›å­¦ä¹ å’Œç ”ç©¶ä½¿ç”¨ï¼Œä½¿ç”¨è€…éœ€è¦éµå®ˆç›¸å…³æ³•å¾‹æ³•è§„å’Œç½‘ç«™ä½¿ç”¨æ¡æ¬¾ã€‚